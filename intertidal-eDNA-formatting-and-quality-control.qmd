---
title: "Formatting and quality control of eDNA metabarcoding data (intertidal dataset)"
format: html
editor: visual
author: Dina-Leigh Simons
toc: true
---

## Load packages

```{r}
#| label: load-packages
#| echo: true
#| results: false
#| message: false
#| warning: false

list.of.packages <- c("dplyr", 
                      "tidyverse", 
                      "phyloseq", 
                      "seqinr", 
                      "dada2", 
                      "sjmisc", 
                      "worrms",
                      "taxize",
                      "tibble", 
                      "taxadb",
                      "reshape2",
                      "decontam",
                      "microViz",
                      "cowplot",
                      "devtools",
                      "qiime2R", 
                      "microbiome",
                      "vegan",
                      "ggforce",
                      "ANCOMBC")

new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages)

invisible(lapply(list.of.packages, library, character.only = TRUE))
```

## Importing data

We'll first load all the required data, including outputs from the taxonomic assignment (.txt files), ASV data (.tsv and .fasta) and metadata (.csv).

### Taxonomic assignments

We BLASTed ASVs against curated databases (MIDORI2 for CO1, Silva for 18S) separately for five regions: Scotland, North and South Wales, Northeast England, and Southwest England.

The BLAST results were then processed using MEtaGenome ANalyzer (MEGAN), which parses the alignments and assigns taxonomy using the Lowest Common Ancestor (LCA) algorithm. This approach ensures accurate taxonomic classification by considering multiple hits while conservatively resolving ambiguous assignments.

```{r}
#| label: load-taxonomy-files
#| warning: false
#| echo: true

#Scotland----
SCH_18S_taxa<- read.table("Input_Data/MEGAN_data/SCH_18S_ASVs_blast_out_SILVA_98-ex.txt", header = F) #Scotland 18S
colnames(SCH_18S_taxa)<- c("ASV", "taxonomy")

SCH_CO1_taxa<- read.table("Input_Data/MEGAN_data/SCH_CO1_ASVs_blast_UNIQ_MIDORI_98-ex.txt", header = F) #Scotland CO1
colnames(SCH_CO1_taxa)<- c("ASV", "taxonomy")

#North wales----
NW_18S_taxa<- read.table("Input_Data/MEGAN_data/NW_18S_ASVs_blast_out_SILVA_98-ex.txt", header = F) #North Wales 18S
colnames(NW_18S_taxa)<- c("ASV", "taxonomy")

NW_CO1_taxa<- read.table("Input_Data/MEGAN_data/NW_CO1_ASVs_blast_UNIQ_MIDORI_98-ex_1per.txt", header = F) #North Wales CO1
colnames(NW_CO1_taxa)<- c("ASV", "taxonomy")

#South Wales----
SW_18S_taxa<- read.table("Input_Data/MEGAN_data/SW_18S_ASVs_blast_out_SILVA_98-ex.txt", header = F) #South Wales 18S
colnames(SW_18S_taxa)<- c("ASV", "taxonomy")

SW_CO1_taxa<- read.table("Input_Data/MEGAN_data/SW_CO1_ASVs_blast_UNIQ_MIDORI_98-ex.txt", header = F) #South Wales CO1
colnames(SW_CO1_taxa)<- c("ASV", "taxonomy")

#Northeast England----
NE_18S_taxa<- read.table("Input_Data/MEGAN_data/NE_18S_ASVs_blast_out_SILVA_98-ex.txt", header = F) #Northeast 18S
colnames(NE_18S_taxa)<- c("ASV", "taxonomy")

NE_CO1_taxa<- read.table("Input_Data/MEGAN_data/NE_CO1_ASVs_blast_UNIQ_MIDORI_98-ex.txt", header = F) #Northwast CO1
colnames(NE_CO1_taxa)<- c("ASV", "taxonomy")

#Southwest England (Cornwall)----
CN_18S_taxa<- read.table("Input_Data/MEGAN_data/CN_18S_ASVs_blast_out_SILVA-1per_98.txt", header = F) #Cornwall 18S
colnames(CN_18S_taxa)<- c("ASV", "taxonomy")

CN_CO1_taxa<- read.table("Input_Data/MEGAN_data/CN_CO1_ASVs_blast_UNIQ_MIDORI-1per_98.txt", header = F) #Cornwall CO1
colnames(CN_CO1_taxa)<- c("ASV", "taxonomy")

#Controls----
Controls_18S_taxa<- read.table("Input_Data/MEGAN_data/Controls_reps_18S_ASVs_blast_out_SILVA_98-ex.txt", header = F) #Controls 18S
colnames(Controls_18S_taxa)<- c("ASV", "taxonomy")

Controls_CO1_taxa<- read.table("Input_Data/MEGAN_data/Repeats_CO1_ASVs_blast_UNIQ_MIDORI_98-ex.txt", header = F) #Controls CO1
colnames(Controls_CO1_taxa)<- c("ASV", "taxonomy")
```

Let's take a look at one of the MEGAN outputs. This file shows the ASVs detected in Scotland derived from the CO1 gene.

```{r}
#| echo: true
#| label: head-taxa
head(SCH_CO1_taxa)
```

### ASV data

Now we import the ASV count and sequence information for each region, obtained using DADA2. We have to do a bit of wrangling to get the fasta file into the correct format.

```{r}
#| label: load-asv-files
#| warning: false
#| echo: true

#Scotland----
SCH_asv_count_18S <- read.table("Input_Data/HPC_processed_data/SCH_18S/06_ASV_counts_SCH_18S.tsv") #Scotland 18S counts
SCH_asv_fasta_18S <- read.fasta("Input_Data/HPC_processed_data/SCH_18S/06_ASV_seqs_SCH_18S.fasta")#Scotland 18S fasta
SCH_asv_fasta_18S_df <- data.frame(ASV=names(SCH_asv_fasta_18S), 
                                   Seqs=unlist(getSequence(SCH_asv_fasta_18S, as.string=T))) #turn into data frame
SCH_asv_fasta_18S_df$Seqs<- toupper(SCH_asv_fasta_18S_df$Seqs) #upper case Seqs

SCH_asv_count_CO1 <- read.table("Input_Data/HPC_processed_data/SCH_COI/06_ASV_counts_SCH_CO1_names.tsv") #Scotland CO1S counts
SCH_asv_fasta_CO1 <- read.fasta("Input_Data/HPC_processed_data/SCH_COI/06_ASV_seqs_SCH_CO1_names.fasta")#Scotland CO1 fasta
SCH_asv_fasta_CO1_df <- data.frame(ASV=names(SCH_asv_fasta_CO1), 
                                   Seqs=unlist(getSequence(SCH_asv_fasta_CO1, as.string=T))) #turn into data frame
SCH_asv_fasta_CO1_df$Seqs<- toupper(SCH_asv_fasta_CO1_df$Seqs) #upper case Seqs

#North Wales----
NW_asv_count_18S <- read.table("Input_Data/HPC_processed_data/NW_18S/06_ASV_counts.tsv") #North Wales 18S counts
NW_asv_fasta_18S <- read.fasta("Input_Data/HPC_processed_data/NW_18S/06_ASV_seqs.fasta")#North Wales 18S fasta
NW_asv_fasta_18S_df <- data.frame(ASV=names(NW_asv_fasta_18S), 
                                  Seqs=unlist(getSequence(NW_asv_fasta_18S, as.string=T))) #turn into data frame
NW_asv_fasta_18S_df$Seqs<- toupper(NW_asv_fasta_18S_df$Seqs) #upper case Seqs

NW_asv_count_CO1 <- read.table("Input_Data/HPC_processed_data/NW_CO1/06_ASV_counts.tsv") #North Wales CO1 counts
NW_asv_fasta_CO1 <- read.fasta("Input_Data/HPC_processed_data/NW_CO1/06_ASV_seqs.fasta")#North Wales CO1 fasta
NW_asv_fasta_CO1_df <- data.frame(ASV=names(NW_asv_fasta_CO1), 
                                  Seqs=unlist(getSequence(NW_asv_fasta_CO1, as.string=T))) #turn into data frame
NW_asv_fasta_CO1_df$Seqs<- toupper(NW_asv_fasta_CO1_df$Seqs) #upper case Seqs

#South Wales----
SW_asv_count_18S <- read.table("Input_Data/HPC_processed_data/SW_18S/06_ASV_counts_SW_18S.tsv") #South Wales 18S counts
SW_asv_fasta_18S <- read.fasta("Input_Data/HPC_processed_data/SW_18S/06_ASV_seqs_SW_18S.fasta") #South Wales 18S fasta
SW_asv_fasta_18S_df <- data.frame(ASV=names(SW_asv_fasta_18S), 
                                  Seqs=unlist(getSequence(SW_asv_fasta_18S, as.string=T))) #turn into data frame
SW_asv_fasta_18S_df$Seqs<- toupper(SW_asv_fasta_18S_df$Seqs) #upper case Seqs

SW_asv_count_CO1 <- read.table("Input_Data/HPC_processed_data/SW_CO1/06_ASV_counts_SW_CO1_names.tsv") #South Wales CO1 counts
SW_asv_fasta_CO1 <- read.fasta("Input_Data/HPC_processed_data/SW_CO1/06_ASV_seqs_SW_CO1_names.fasta") #South Wales CO1 fasta
SW_asv_fasta_CO1_df <- data.frame(ASV=names(SW_asv_fasta_CO1), 
                                  Seqs=unlist(getSequence(SW_asv_fasta_CO1, as.string=T))) #turn into data frame
SW_asv_fasta_CO1_df$Seqs<- toupper(SW_asv_fasta_CO1_df$Seqs) #upper case Seqs

#Northeast----
NE_asv_count_18S <- read.table("Input_Data/HPC_processed_data/NE_18S/06_ASV_counts_NE.tsv") #Northeast 18S counts
NE_asv_fasta_18S <- read.fasta("Input_Data/HPC_processed_data/NE_18S/06_ASV_seqs_NE.fasta")#Northeast 18S fasta
NE_asv_fasta_18S_df <- data.frame(ASV=names(NE_asv_fasta_18S), 
                                  Seqs=unlist(getSequence(NE_asv_fasta_18S, as.string=T))) #turn into data frame
NE_asv_fasta_18S_df$Seqs<- toupper(NE_asv_fasta_18S_df$Seqs) #upper case Seqs

NE_asv_count_CO1 <- read.table("Input_Data/HPC_processed_data/NE_CO1/06_ASV_counts.tsv") #Northeast CO1 counts
NE_asv_fasta_CO1 <- read.fasta("Input_Data/HPC_processed_data/NE_CO1/06_ASV_seqs.fasta")#Northeast CO1 fasta
NE_asv_fasta_CO1_df <- data.frame(ASV=names(NE_asv_fasta_CO1), 
                                  Seqs=unlist(getSequence(NE_asv_fasta_CO1, as.string=T))) #turn into data frame
NE_asv_fasta_CO1_df$Seqs<- toupper(NE_asv_fasta_CO1_df$Seqs) #upper case Seqs

#Cornwall----
CN_asv_count_18S <- read.table("Input_Data/HPC_processed_data/CN_18S/06_ASV_counts.tsv") #Cornwall 18S counts
CN_asv_fasta_18S <- read.fasta("Input_Data/HPC_processed_data/CN_18S/06_ASV_seqs.fasta")#Cornwall 18S fasta
CN_asv_fasta_18S_df <- data.frame(ASV=names(CN_asv_fasta_18S), 
                                  Seqs=unlist(getSequence(CN_asv_fasta_18S, as.string=T))) #turn into data frame
CN_asv_fasta_18S_df$Seqs<- toupper(CN_asv_fasta_18S_df$Seqs) #upper case Seqs

CN_asv_count_CO1 <- read.table("Input_Data/HPC_processed_data/CN_CO1/06_ASV_counts.tsv") #Cornwall CO1 counts
CN_asv_fasta_CO1 <- read.fasta("Input_Data/HPC_processed_data/CN_CO1/06_ASV_seqs.fasta")#Cornwall CO1 fasta
CN_asv_fasta_CO1_df <- data.frame(ASV=names(CN_asv_fasta_CO1), 
                                  Seqs=unlist(getSequence(CN_asv_fasta_CO1, as.string=T))) #turn into data frame
CN_asv_fasta_CO1_df$Seqs<- toupper(CN_asv_fasta_CO1_df$Seqs) #upper case Seqs

#Controls
controls_PhD2_asv_count_18S <- read.table("Input_Data/HPC_processed_data/Controls_18S/06_ASV_counts_Controls.tsv") #controls 18S counts
controls_PhD2_asv_fasta_18S <- read.fasta("Input_Data/HPC_processed_data/Controls_18S/06_ASV_seqs_Controls.fasta")#controls 18S fasta
controls_PhD2_asv_fasta_18S_df <- data.frame(ASV=names(controls_PhD2_asv_fasta_18S), 
                                             Seqs=unlist(getSequence(controls_PhD2_asv_fasta_18S, as.string=T))) #turn into data frame
controls_PhD2_asv_fasta_18S_df$Seqs<- toupper(controls_PhD2_asv_fasta_18S_df$Seqs) #upper case Seqs

controls_PhD2_asv_count_CO1 <- read.table("Input_Data/HPC_processed_data/Repeats_CO1/06_ASV_counts.tsv") #controls CO1 counts
controls_PhD2_asv_fasta_CO1 <- read.fasta("Input_Data/HPC_processed_data/Repeats_CO1/06_ASV_seqs.fasta")#controls  CO1 fasta
controls_PhD2_asv_fasta_CO1_df <- data.frame(ASV=names(controls_PhD2_asv_fasta_CO1), 
                                             Seqs=unlist(getSequence(controls_PhD2_asv_fasta_CO1, as.string=T))) #turn into data frame
controls_PhD2_asv_fasta_CO1_df$Seqs<- toupper(controls_PhD2_asv_fasta_CO1_df$Seqs) #upper case Seqs

```

Let's take a look at an example of the two imported ASV files. Again, these shows the ASVs detected in Scotland derived from the CO1 gene.

```{r}
#| echo: true
#| label: str-ASV
str(SCH_asv_count_CO1) #counts
```

```{r}
#| echo: true
#| label: head-ASV
head(SCH_asv_fasta_CO1_df) #sequences
```

### Metadata

Finally, let's import any metadata associated to our samples.

```{r}
#| label: load-metadata
sites <- read.csv("Input_Data/Metadata/eDNA_Site_Rockpool_Data_Oct2023.csv", na.strings=c("","NA"))
IDs_stageone <- read.csv("Input_Data/Metadata/sample_ID_matching_stageone.csv")
IDs_stagetwo <- read.csv("Input_Data/Metadata/sample_ID_matching_stagetwo.csv")
```

And let's take a quick look at the metadata files too. You can see there are 63 variables which are associated to our samples.

```{r}
#| label: str-meta
str(sites)
```

We also have some data to help tidy up sample IDs later in the pipeline.

```{r}
#| label: head-IDs
head(IDs_stageone) #labels
```

## Data wrangling to create a single data set

We have lots of different files here which need to be manipulated into one single data frame for each primer. A few steps are required to achieve this.

### Merge taxonomic assignments and ASV information

```{r}
#| label: merge-ASV-Seq

#Scotland----
SCH_asv_18S_merged <- merge(SCH_asv_fasta_18S_df, SCH_asv_count_18S, by.x = "Seqs", by.y = "row.names") #add ASVs to count data
SCH_taxa_18S <- full_join(SCH_18S_taxa, SCH_asv_18S_merged, by = "ASV") #full_join keeps ASVs which didn't get assigned, left_join would remove unassigned

SCH_asv_CO1_merged <- merge(SCH_asv_fasta_CO1_df, SCH_asv_count_CO1, by.x = "Seqs", by.y = "row.names") #add ASVs to count data
SCH_taxa_CO1 <- full_join(SCH_CO1_taxa, SCH_asv_CO1_merged, by = "ASV") #full_join keeps ASVs which didn't get assigned, left_join would remove unassigned

#North Wales----
NW_asv_18S_merged <- merge(NW_asv_fasta_18S_df, NW_asv_count_18S, by.x = "Seqs", by.y = "row.names") #add ASVs to count data
NW_taxa_18S <- full_join(NW_18S_taxa, NW_asv_18S_merged, by = "ASV") #full_join keeps ASVs which didn't get assigned, left_join would remove unassigned

NW_asv_CO1_merged <- merge(NW_asv_fasta_CO1_df, NW_asv_count_CO1, by.x = "Seqs", by.y = "row.names") #add ASVs to count data
NW_taxa_CO1 <- full_join(NW_CO1_taxa, NW_asv_CO1_merged, by = "ASV") #full_join keeps ASVs which didn't get assigned, left_join would remove unassigned

#South Wales----
SW_asv_18S_merged <- merge(SW_asv_fasta_18S_df, SW_asv_count_18S, by.x = "Seqs", by.y = "row.names") #add ASVs to count data
SW_taxa_18S <- full_join(SW_18S_taxa, SW_asv_18S_merged, by = "ASV") #full_join keeps ASVs which didn't get assigned, left_join would remove unassigned

SW_asv_CO1_merged <- merge(SW_asv_fasta_CO1_df, SW_asv_count_CO1, by.x = "Seqs", by.y = "row.names") #add ASVs to count data
SW_taxa_CO1 <- full_join(SW_CO1_taxa, SW_asv_CO1_merged, by = "ASV") #full_join keeps ASVs which didn't get assigned, left_join would remove unassigned

#Northeast----
NE_asv_18S_merged <- merge(NE_asv_fasta_18S_df, NE_asv_count_18S, by.x = "Seqs", by.y = "row.names") #add ASVs to count data
NE_taxa_18S <- full_join(NE_18S_taxa, NE_asv_18S_merged, by = "ASV") #full_join keeps ASVs which didn't get assigned, left_join would remove unassigned

NE_asv_CO1_merged <- merge(NE_asv_fasta_CO1_df, NE_asv_count_CO1, by.x = "Seqs", by.y = "row.names") #add ASVs to count data
NE_taxa_CO1 <- full_join(NE_CO1_taxa, NE_asv_CO1_merged, by = "ASV") #full_join keeps ASVs which didn't get assigned, left_join would remove unassigned

#Cornwall----
CN_asv_18S_merged <- merge(CN_asv_fasta_18S_df, CN_asv_count_18S, by.x = "Seqs", by.y = "row.names") #add ASVs to count data
CN_taxa_18S <- full_join(CN_18S_taxa, CN_asv_18S_merged, by = "ASV") #full_join keeps ASVs which didn't get assigned, left_join would remove unassigned

CN_asv_CO1_merged <- merge(CN_asv_fasta_CO1_df, CN_asv_count_CO1, by.x = "Seqs", by.y = "row.names") #add ASVs to count data
CN_taxa_CO1 <- full_join(CN_CO1_taxa, CN_asv_CO1_merged, by = "ASV") #full_join keeps ASVs which didn't get assigned, left_join would remove unassigned

#Controls----
Controls_asv_18S_merged <- merge(controls_PhD2_asv_fasta_18S_df, controls_PhD2_asv_count_18S, by.x = "Seqs", by.y = "row.names") #add ASVs to count data
Controls_taxa_18S <- full_join(Controls_18S_taxa, Controls_asv_18S_merged, by = "ASV") #full_join keeps ASVs which didn't get assigned, left_join would remove unassigned

Controls_asv_CO1_merged <- merge(controls_PhD2_asv_fasta_CO1_df, controls_PhD2_asv_count_CO1, by.x = "Seqs", by.y = "row.names") #add ASVs to count data
Controls_taxa_CO1 <- full_join(Controls_CO1_taxa, Controls_asv_CO1_merged, by = "ASV") #full_join keeps ASVs which didn't get assigned, left_join would remove unassigned
```

### Remove undetermined sequences in 18S data

Sequences that were unable to be assigned to a specific sample in the 18S are stored under the variable 'S0'. We want to remove those sequences from our data.

```{r}
#| label: remove-undetermined-seqs

drop <- c("S0") # set which variable we want to remove (undetermined in MiniSeq run)

SCH_taxa_18S <- SCH_taxa_18S[,!(names(SCH_taxa_18S) %in% drop)]
NW_taxa_18S <- NW_taxa_18S[,!(names(NW_taxa_18S) %in% drop)]
SW_taxa_18S <- SW_taxa_18S[,!(names(SW_taxa_18S) %in% drop)] 
NE_taxa_18S <- NE_taxa_18S[,!(names(NE_taxa_18S) %in% drop)]
CN_taxa_18S <- CN_taxa_18S[,!(names(CN_taxa_18S) %in% drop)]
Controls_taxa_18S <- Controls_taxa_18S[,!(names(Controls_taxa_18S) %in% drop)]

```

### Convert to long format

Here, we flip the data sets into a long format and add a region variable.

```{r}
#| label: convert-long

#Scotland----
flip_SCH_18S <- gather(SCH_taxa_18S, sample, reads, S66:S132) 
flip_SCH_18S$region <- "Scotland"

flip_SCH_CO1 <- gather(SCH_taxa_CO1, sample, reads, Adapter3616.SCH01.04rep:Adapter453.SCHNC2803) #this is not number order, this is first and last column
flip_SCH_CO1$region <- "Scotland"

#North Wales----
flip_NW_18S <- gather(NW_taxa_18S, sample, reads, S1:S51) 
flip_NW_18S$region <- "North Wales"

flip_NW_CO1 <- gather(NW_taxa_CO1, sample, reads, X1.NW01.01:X9.NW02.06) #this is not number order, this is first and last column
flip_NW_CO1$region <- "North Wales"

#South Wales----
flip_SW_18S <- gather(SW_taxa_18S, sample, reads, S60:S54) 
flip_SW_18S$region <- "South Wales"

flip_SW_CO1 <- gather(SW_taxa_CO1, sample, reads, Adapter3553.SW01.01:Adapter3620.PCrep) #this is not number order, this is first and last column
flip_SW_CO1$region <- "South Wales"

#Northeast----
flip_NE_18S <- gather(NE_taxa_18S, sample, reads, S73:S123) 
flip_NE_18S$region <- "Northeast England"

flip_NE_CO1 <- gather(NE_taxa_CO1, sample, reads, X100.NE05.02:X99.NE05.01) #this is not number order, this is first and last column
flip_NE_CO1$region <- "Northeast England"

#Cornwall----
flip_CN_18S <- gather(CN_taxa_18S, sample, reads, S133:S183) 
flip_CN_18S$region <- "Cornwall"

flip_CN_CO1 <- gather(CN_taxa_CO1, sample, reads, X115.CN01.01:X192.CN01.02rep) #this is not number order, this is first and last column
flip_CN_CO1$region <- "Cornwall"

#Controls----
flip_controls_18S <- gather(Controls_taxa_18S, sample, reads, S34:S61) #this is not number order, this is first and last column
flip_controls_18S$region <- "Varied"

flip_controls_CO1 <- gather(Controls_taxa_CO1, sample, reads, X173.SCH02.03:X195.minusVEpcr) #this is not number order, this is first and last column
flip_controls_CO1$region <- "Varied"

```

Let's take a look at the long format for Scotland CO1. We now have a long format data set with ASVs, taxonomy, sequences, sample names, number of reads and region variables.

```{r}
#| label: head-flipped
head(flip_SCH_CO1)
```

### Join data frames

We will merge our data in multiple stages: first by sequencing run, then by gene, and finally, we will perform the final join. Once we have our single data frame, we'll add in our metadata.

Our data was derived from multiple sequences runs. Therefore, we are going to join the data in the same sequencing run first. This means we can correctly match IDs we want to sample names.

```{r}
#| label: join-seq-run

# Merge stage-one locations (SCH and SW) and add IDs
full_stageone_18S <-rbind(flip_SCH_18S, flip_SW_18S) %>% 
  dplyr::rename(sample_18S = sample) %>% #rename sample column to match
  full_join(IDs_stageone, by = "sample_18S")

full_stageone_18S$region <- as.factor(full_stageone_18S$region)

full_stageone_CO1 <-rbind(flip_SCH_CO1, flip_SW_CO1) %>% 
  dplyr::rename(sample_CO1 = sample) %>% #rename sample column to match
  full_join(IDs_stageone, by = "sample_CO1")

full_stageone_CO1$region <- as.factor(full_stageone_CO1$region)

# Merge stage-two locations (controls, CN, NE, NW)
full_stagetwo_18S <-rbind(flip_controls_18S, flip_CN_18S, flip_NE_18S, flip_NW_18S) %>% 
  dplyr::rename(sample_18S = sample) %>% 
  left_join(IDs_stagetwo, by = "sample_18S")

full_stagetwo_18S$region <- as.factor(full_stagetwo_18S$region)

full_stagetwo_CO1 <-rbind(flip_controls_CO1, flip_CN_CO1, flip_NE_CO1, flip_NW_CO1) %>%
  dplyr::rename(sample_CO1 = sample) %>% 
  full_join(IDs_stagetwo, by = "sample_CO1")

full_stagetwo_CO1$region <- as.factor(full_stagetwo_CO1$region)
```

Now we join data sets by primer and add primer variable. This will leave us with two main data sets, one for 18S and one for CO1. We also want to make sure out wrangling worked as expected by checking there are no duplicate rows.

```{r}
#| label: join-primer

full_18S <- rbind(full_stageone_18S, full_stagetwo_18S) #18s
full_18S$primer <- "18S"
any(duplicated(full_18S)) #check for duplicates - should be false

full_CO1 <- rbind(full_stageone_CO1, full_stagetwo_CO1) #CO1
full_CO1$primer <- "CO1"
any(duplicated(full_CO1)) #check for duplicates - should be false
```

Finally, we join the two primer data sets into a single data set.

```{r}
#| label: join-final

eDNA_ASVs_long <- rbind(full_18S, full_CO1)
any(duplicated(eDNA_ASVs_long)) #check for duplicates - should be false (this may take a minute or so)
```

Let's now add in our metadata and do some final tweaks to sample names.

```{r}
#| label: add-metadata

eDNA_long_data <- eDNA_ASVs_long %>% 
  subset(select = -c(sample_18S, sample_CO1)) %>% 
  dplyr::rename(fieldID = sample_ID)
eDNA_long_data <- full_join(eDNA_long_data, sites, by = "fieldID")

eDNA_long_data$fullID = paste(eDNA_long_data$fieldID, eDNA_long_data$primer, sep="_") #adds CO1 or 18S onto the end of the fieldID for further analyses
```

Great - we now have a single long data set with all ASV across samples and genes, as well as associated metadata.

```{r}
#| label: str-long-data

str(eDNA_long_data)
```

## Cleaning taxonomic names

Now we have our data, we need to tidy up our taxonomic names and get more information on the taxa we've detected.

### Remove ambiguous names

We first remove any occurrences where the taxonomic matches certain ambiguous phrases. These assignment tell us nothing about the organism we've detected, so it's best to remove them.

```{r}
#| label: remove-ambiguous-names

unassigned_phrases <- c("uncultured", "Uncultured", "unclassified", "Unclassified", "NCBI", 
             "pseudo", "Pseudo", "Not assigned", "not assigned", "SAR", "sar", 
             "cellular", "Cellular", "environmental", "Environemental", 
             "eukaryote", "Eukaryote", "eukaryota", "Eukaryota")

eDNA_long_data_cleaning <- eDNA_long_data[!grepl(paste(unassigned_phrases, collapse = "|"), eDNA_long_data$taxonomy),] %>%
  drop_na(taxonomy)

```

### Using clean_names() in janitor package

We often get random spaces or phrases we don't want in our taxonomic names. We can use the [janitor package](https://cran.r-project.org/web/packages/janitor/vignettes/janitor.html) to clean our taxonomic names.

```{r}
#| message: false
#| label: clean-names-janitor

names <- unique(eDNA_long_data_cleaning$taxonomy) #get all unique names from the data
cleaned_names <- clean_names(names) #clean names using janitor package in tidyverse
names_together <- data.frame(names, cleaned_names) %>% 
  dplyr::rename(taxonomy = names, taxonomy_clean = cleaned_names) %>% #join cleaned names next to original
  mutate(taxonomy_clean = gsub(' cf','', taxonomy_clean)) %>% #remove cfs which function didn't remove
  mutate(taxonomy_clean = gsub('cf ','', taxonomy_clean)) #remove cfs which function didn't remove

eDNA_long_data_cleaned <- full_join(names_together, eDNA_long_data_cleaning)

```

We should now have a variable added to our data set called taxonomy_clean. We will use this variable as the taxonomic name moving forward.

```{r}
#| label: str-long-data-clean

str(eDNA_long_data_cleaned)
```

We can see the data set is a lot smaller after removal of rows with ambiguous names.

## Extracting information from WoRMS

The [World Register of Marine Species](https://www.marinespecies.org/) (WoRMS) is an authoritative classification and catalogue of marine names. It can provide a range of information. We will be extracting information on taxonomy and functional trait information using the taxize and worrms functions.

### Defining functions

First, we've got to set-up some functions.

The first function called `get_wormsid_noerror` extracts unique identifiers for all taxon used in WoRMS called AphiaIDs.

```{r}
#| label: define-function-wormsid

get_wormsid_noerror <- function(sp_name){
  wm_id <- try(taxize::get_wormsid(sp_name,
                                   accepted = TRUE, 
                                   searchtype= "scientific", 
                                   marine_only = FALSE,
                                   ask = FALSE,
                                   row = 1,
                                   message = FALSE),
               silent = TRUE)
  
  #remove end word to attempt matching genus when previous no match
  
  if(class(wm_id) == "try-error"){
    sp_name <- str_remove(sp_name, "(\\s+\\w+)") 
    wm_id <- try(taxize::get_wormsid(sp_name,
                                     accepted = TRUE, 
                                     searchtype= "scientific", 
                                     marine_only = FALSE,
                                     ask = FALSE,
                                     row = 1,
                                     message = FALSE),
                 silent = TRUE)
  } 
  
  #convert non-matched species to NA AphiaID
  
  if(class(wm_id) == "try-error"){
    wm_id <- NA
  } 
  tibble(sciname = sp_name, aphiaID = as.double(unlist(wm_id)))
}
```

The second function called `get_wormsmeta` accesses and formats any taxonomic meta data based on Aphia IDs.

```{r}
#| label: define-function-wormsmeta

get_wormsmeta <- function(aphia_input){
  
  #split into smaller chunks for wm_record() to work
  taxadf_split <- split(aphia_input, (seq(nrow(aphia_input))-1) %/% 50) #split into smaller groups for worms to work
  temp_df <- data.frame()
  
  #run wm_record() through split list
  for (i in taxadf_split) {
    taxa_temp <- wm_record(id = i$aphiaID)
    temp_df = rbind(temp_df, taxa_temp)
  }
  tibble(temp_df)
}
```

The third function called `get_worms_fgrp` (originally developed [here](https://github.com/tomjwebb/WoRMS-functional-groups)) accesses and formats information on broad taxonomic groupings of taxa based on Aphia IDs.

```{r}
#| label: define-function-worms-fgrp

get_worms_fgrp <- function(AphiaID) {
  
  #' First, test if the species has attribute data
  attr_dat <- try(wm_attr_data(
    AphiaID, include_inherited = TRUE), silent = TRUE)
  
  #' set up out as null for later use
  out <- NULL
  
  if(!identical(class(attr_dat), "try-error")){
    
    #' if attribute data exists, test if functional group is there
    if("Functional group" %in% attr_dat$measurementType){
      fg_dat <- attr_dat %>% filter(measurementType == "Functional group")
      
      #' Insert if statement here about $children empty
      #' assign the $children - so that it can be used 
      children <- fg_dat$children
      
      if(max(lengths(children)) > 0 ){
        #' Extract the life stage information from the $children field
        life_stage <- children %>% bind_rows() %>%
          dplyr::select(measurementValue) %>%
          dplyr::rename(stage = measurementValue)
        
        #' add in rows for instances missing children
        idx <- which(lengths(children) == 0)
        if(length(idx) > 0){
          life_stage_null <- tibble(stage = rep(as.character(NA), length(children)))
          idy <- (1:length(children))[-idx]
          life_stage_null[idy,] <- life_stage
          life_stage <- life_stage_null
        }
        life_stage <- life_stage %>% bind_cols(., fg_dat)
        
        #' deal with cases where multiple records are returned for the same life stage:
        #' add a suffix to subsequent identical stages (adult_2, etc.)        
        life_stage <- life_stage %>% group_by(stage) %>% dplyr::mutate(nth_stage_val = 1:n()) %>% 
          ungroup() %>% 
          mutate(stage = case_when(
            nth_stage_val == 1 ~ stage,
            TRUE ~ paste(stage, nth_stage_val, sep = "_")
          )
          ) %>% 
          select(-nth_stage_val)
        
        #' create the output to return
        out <- tibble(AphiaID = as.numeric(life_stage$AphiaID),
                      stage = life_stage$stage, fun_grp = life_stage$measurementValue)
        
      } else{
        #' If no life stage info, assume stage is adult
        out <- tibble(AphiaID = as.numeric(fg_dat$AphiaID),
                      stage = "adult", fun_grp = fg_dat$measurementValue)
        
        #' Deal with cases where multiple records are returned (i.e. 2 or more adult fun_grps)
        if(nrow(out) > 1){
          out <- out %>% group_by(stage) %>% dplyr::mutate(nth_stage_val = 1:n()) %>% 
            ungroup() %>% 
            mutate(stage = case_when(
              nth_stage_val == 1 ~ stage,
              TRUE ~ paste(stage, nth_stage_val, sep = "_")
            )
            ) %>% 
            select(-nth_stage_val)
        }
      } 
    }
    
    #' add Pisces, from paraphyletic group: this takes priority over the above
    #' (e.g. class something as Pisces if it is a fish even if it is also listed as benthos)
    if ("Paraphyletic group" %in% attr_dat$measurementType) {
      if(first(
        attr_dat$measurementValue[attr_dat$measurementType == "Paraphyletic group"] == "Pisces")){
        out <- tibble(AphiaID = AphiaID, stage = "adult",
                      fun_grp = first(attr_dat$measurementValue[
                        attr_dat$measurementType == "Paraphyletic group"]))
      }
    }
  }
  #' add other paraphyletic groups: Algae, Algae > Macroalgae, Mangroves this DOES NOT takes priority over the above; i.e. only use if a functional group (e.g. phytoplankton) is not available
  if(is.null(out)){
    if (!identical(class(attr_dat), "try-error") && "Paraphyletic group" %in% attr_dat$measurementType) {
      out <- tibble(AphiaID = AphiaID, stage = "adult",
                    fun_grp = first(attr_dat$measurementValue[
                      attr_dat$measurementType == "Paraphyletic group"]))
    }
  }
  
  #' check taxonomy for other groups
  if(is.null(out)){
    taxo_dat <- try(wm_classification(AphiaID), silent = TRUE)
    if(identical(class(taxo_dat), "try-error")){
      fg <- as.character(NA)
    } else {
      fg <- case_when(
        "Aves" %in% taxo_dat$scientificname ~ "birds",
        "Mammalia" %in% taxo_dat$scientificname ~ "mammals",
        "Reptilia" %in% taxo_dat$scientificname ~ "reptiles",
        TRUE ~ as.character(NA)
      )
    }
    out <- tibble(AphiaID = AphiaID, stage = "adult", fun_grp = fg)
  }
  
  #' what if there are duplicate rows?
  out <- out[!duplicated(out), ]
  out <- out[!(is.na(out$stage)),] #gets rid of NA stages
  
  #' Replace 'not applicable' with NA
  out <- out %>% mutate_all(~ifelse(. == 'not applicable', NA, .))
  
  #' Keep only the life stage with the highest number (if applicable)
  out <- out %>%
    separate(stage, into = c("stage", "number"), sep = "_", remove = FALSE) %>%
    group_by(AphiaID, stage) %>%
    arrange(desc(number)) %>%
    slice(1) %>%
    ungroup() %>%
    select(-number)
  
  #' Convert specific life stages to 'larva_other'
  out <- out %>% mutate(stage = case_when(
    stage %in% c('cerinula', 'larva > planula', 'medusa', 'zoea', 'nauplius', 'polyp', 'medusoid', 'ephyra', 'colony') ~ 'larva_other',
    TRUE ~ stage
  ))
  
  #' do some tidying of the output: tidy up functional_group text
  #' and spread to give one column per life stage
  out <- out %>% 
    distinct(AphiaID, stage, .keep_all = TRUE) %>%  # Add this line to remove duplicates
    mutate(functional_group = case_when(
      str_detect(fun_grp, ">") ~ tolower(word(fun_grp, -1)),
      fun_grp == "Pisces" ~ "fish",
      fun_grp == "Algae > Macroalgae" ~ "macroalgae",
      TRUE ~ tolower(fun_grp)
    )) %>%
    dplyr::select(-fun_grp) %>%
    spread(stage, functional_group)
  
  #' Change column name NA to other
  colnames(out)[colnames(out) == "NA"] <- "other"
  
  #' output the fg data
  out
}
```

Before we run anything, we need to extract the taxa names we want to run through the functions.

```{r}
#| label: get-taxa-names

eDNA_taxa_unique_list <- unique(eDNA_long_data_cleaned$taxonomy_clean) #Get unique taxa for WoRMs input as list
eDNA_taxa_unique_df <- data.frame(taxonomy = tolower(unique(eDNA_long_data_cleaned$taxonomy_clean)), 
                                  stringsAsFactors = FALSE) ##Get unique taxa for WoRMs input as df

head(eDNA_taxa_unique_df)
```

### Get Aphia IDs

Let's use the function `get_wormsid()` to obtain Aphia IDs for our taxa.

The duration of this task is approximately 15-minutes for this data set, but can vary depending on the size of the data. Any non-matched species will produce a NA.

To prevent the document taking hours to render, we've put \# in front of the code for now. We've saved the output and reloaded the data back in.

```{r}
#| label: get-worms-alphia

#aphiaID_worms_output <- eDNA_taxa_unique_list %>%
  #purrr::map(get_wormsid_noerror, .progress = TRUE) %>%
  #enframe() %>%
  #unnest(cols = everything()) %>%
  #select(-name)
```

We'll save this output to reduce time when rerunning.

```{r}
#| label: write-worms-alphia

#write.csv(aphiaID_worms_output, "Processed_Data/aphiaID_worms_output.csv")
```

Let's reload the data back in and do some formatting.

```{r}
#| label: reload--worms-alphia

aphiaID_worms_output <- read.csv("Processed_Data/aphiaID_worms_output.csv", row.names = 1)

aphiaID_worms_output$taxonomy_clean = eDNA_taxa_unique_df$taxonomy #add previous taxonomy name for joining later (order remains the same)
aphiaID_worms_output$aphiaID = as.integer(aphiaID_worms_output$aphiaID) #convert to integer for joining

head(aphiaID_worms_output)
```

Let's check how many matches we made.

```{r}
#| echo: false
#| label: match-worms-alphia
#| 
print(paste("You matched", nrow(eDNA_taxa_unique_df) - sum(is.na(aphiaID_worms_output$aphiaID)),"/", nrow(eDNA_taxa_unique_df), "taxa with AphiaIDs. Therefore,", sum(is.na(aphiaID_worms_output$aphiaID)), "entries could not be matched and do not have AphiaIDs."))
```

Now let's add the Aphia IDs to our main data set.

```{r}
#| label: join-worms-alphia

eDNA_long_data_final <- full_join(eDNA_long_data_cleaned, aphiaID_worms_output, by = join_by(taxonomy_clean))

str(eDNA_long_data_final)
```

### Get taxonomic metadata

Next, let's use the function `get_wormsmeta()` to obtain taxonomic information for our Aphia IDs. This command takes a few minutes to run.

```{r}
#| label: get-worms-meta

worms_meta_output <- get_wormsmeta(aphiaID_worms_output)
```

Let's look at the output. We can see we now have lots of information for each taxon.

```{r}
#| label: view-worms-meta

str(worms_meta_output)
```

A bit of tidying of this output is required to be able to join it with our own.

```{r}
#| label: tidy-worms-meta

worms_meta_output_tidy <- worms_meta_output %>%
  dplyr::rename(aphiaID = AphiaID) %>%
  filter(!is.na(aphiaID)) %>% #remove NAs
  distinct() #remove duplicates
```

Now let's add the taxonomic metadata to our main data set and check the joining of data sets worked.

```{r}
#| label: join-worms-meta

eDNA_long_data_final <- right_join(eDNA_long_data_final, worms_meta_output_tidy, by = "aphiaID") #worms meta

which(is.na(eDNA_long_data_final$aphiaID), arr.ind=TRUE) #check NAs in IDs
any(duplicated(eDNA_long_data_final)) #check for duplicates
```

### Get broad functional groups

Finally, let’s use the function `get_worms_fgrp()` to obtain the broad taxonomic groups of our taxa.

Similar to above, this command takes a long time (approximately 30 minutes with this data set). We have put \# in front of this code to reduce time to render.

```{r}
#| label: get-worms-fgrp

species <- tibble(AphiaID = c(unique(eDNA_long_data_final$aphiaID))) #list aphiaIDs

#worms_fgrp_output <- species %>%
  #group_by(AphiaID) %>%
  #do(get_worms_fgrp(AphiaID = .$AphiaID))
```

We'll save this output to reduce time when rerunning.

```{r}
#| label: write-worms-fgrp  

#write.csv(worms_fgrp_output, "Processed_Data/worms_fgrp_output.csv")
```

Let's reload the data back in.

```{r}
#| label: reload-worms-fgrp  

worms_fgrp_output <- read.csv("Processed_Data/worms_fgrp_output.csv", row.names = 1)
```

Let's see how many matches we got.

```{r}
#| label: match_worms-fgrp 
#| echo: false 

print(paste("You found function groups for", nrow(worms_fgrp_output) - sum(is.na(worms_fgrp_output$adult)),"/", nrow(worms_fgrp_output), "adult taxa. Therefore,", sum(is.na(worms_fgrp_output$adult)), "have no functional group."))
```

Now let's add the functional group information to our main data set and check the joining of data sets worked.

```{r}
#| label: join-worms-fgrp
#| message: false

worms_fgrp_output_tidy<- worms_fgrp_output %>% dplyr::rename(aphiaID = AphiaID)
eDNA_long_data_final <- left_join(eDNA_long_data_final, worms_fgrp_output_tidy)
```

Quick look at the final long data set (remember this has not been decontaminated yet!).

```{r}
#| label: str-long-data-final

str(eDNA_long_data_final)
```

## Create phyloseq object

We've now got a tidied long data set with lots of meta data. To aid further check and analyses, we can convert our long data into a `phyloseq` object to be used in the `phyloseq` package.

First, we need to document and remove any failed samples.

```{r}
#| label: remove-failed-samples  

failed_samples <- c("PHD1-SCH02-03_CO1",
                    "PHD1-SCH02-05_CO1",
                    "PHD1-SCH03-05rep_CO1",
                    "PHD1-SCH04-03_CO1",
                    "PHD1-SCH06-05_18S",
                    "PHD1-SW04-01_CO1",
                    "PHD1-SW06-02_18S",
                    "PHD2-CN02-03_18S",
                    "PHD2-CN05-02_18S",
                    "PHD2-NE01-09_CO1",
                    "PHD2-NE02-03_CO1",
                    "PHD2-NE04-03_18S",
                    "PHD2-NW03-01_CO1")

eDNA_long_data_removefailed <- filter(eDNA_long_data_final, !fullID %in% failed_samples)
```

### Create tax_table()

We need to extract all the taxonomic information from our long data and do some tidying.

```{r}
#| label: format-tax-table  

taxa <- unique(subset(eDNA_long_data_removefailed, select = c("kingdom", 
                                             "phylum", 
                                             "class", 
                                             "order", 
                                             "family", 
                                             "genus", 
                                             "valid_name",
                                             "adult", #adult is functional group
                                             "valid_AphiaID",
                                             "rank"))) 

taxa$valid_name <- as.factor(taxa$valid_name)
taxa <- taxa[order(taxa$valid_name),] #order alphabetically to match other ps elements
rownames(taxa) <-taxa$valid_name
taxa <- as.matrix(taxa)
```

Now, save the taxa matrix as a tax_table() object.

```{r}
#| label: create-tax-table   

phylo_tax <- tax_table(taxa)
```

### Create sample_data()

Similarly, need to extract all the relavent sample information from our long data and do some tidying.

```{r}
#| label: format-sample-table

meta <- unique(subset(eDNA_long_data_removefailed, select = c("fieldID",
                                             "projectID",
                                             "eventID",
                                             "type", 
                                             "verbatimLocality",
                                             "localityID",
                                             "shorePosition", 
                                             "fullID",
                                             "year",
                                             "month",
                                             "day",
                                             "eventTime",
                                             "dna_concentration_CO1",
                                             "dna_concentration_18S",
                                             "negCheckLogical",
                                             "pairedRepLogical",
                                             "country",
                                             "controlCheck",
                                             "sampleType",
                                             "primer",
                                             "date_amplified_CO1",
                                             "date_amplified_18S",
                                             "batch_extraction",
                                             "batch_PCR_CO1",
                                             "plate_col_CO1",
                                             "plate_row_CO1",
                                             "batch_PCR_18S",
                                             "plate_col_18S",
                                             "plate_row_18S",
                                             "date_extraction",
                                             "exposure",
                                             "weather",
                                             "tide",
                                             "lowWater",
                                             "decimalLongitude",
                                             "decimalLatitude",
                                             "eventTime",
                                             "rockpoolarea.cm.2.",
                                             "averageDepth..cm.",
                                             "rockpoolVol.m.3.",
                                             "averagePH",
                                             "averageTemp"
)))

meta <- meta[order(meta$fullID),] #order rows alphabetically
rownames(meta) <-meta$fullID
```

Now, save the sample matrix as a sample_data() object.

```{r}
#| label: create-sample-table

phylo_samples <- sample_data(meta)
```

### Create otu_table()

Since our data are currently in long format, we need to recreate the ASV matrix through some data wrangling.

```{r}
#| label: format-otu-table-long

species_reads_long <- eDNA_long_data_removefailed %>%
  select(ASV, reads, fullID) %>%
  mutate(across(c(ASV, fullID), as.factor)) %>%
  distinct()

str(species_reads_long)
which(is.na(species_reads_long)) #check for NAs
```

Turning our long data into a wide format takes a long time due to the size of the data.

```{r}
#| label: compile-otu-table-wide

species_reads_wide <- species_reads_long %>% 
  pivot_wider(names_from = fullID, 
              values_from = reads, 
              values_fill = list(reads = 0)) %>% 
  as.data.frame()
```

A bit of formatting is needed for joining elements together later on. We can see that we have 6321 ASVs and 655 samples.

```{r}
#| label: format-otu-table-wide

#order names alphabetically
species_reads_wide<- species_reads_wide[order(species_reads_wide$ASV),] #order rows alphabetically
species_reads_wide <- species_reads_wide[,order(colnames(species_reads_wide))] #order cols alphabetically

#update row names to ASVs
rownames(species_reads_wide) <- NULL
species_reads_wide <- species_reads_wide %>% column_to_rownames(var="ASV")

dim(species_reads_wide)
```

We can also repeat the last few steps but group by `valid_names` to get a taxa-level `otu_table` . In my further analyses, taxa-level detection are appropriate and so we will move forward using this `otu_table`.

We can see that we have 1780 ASVs and 655 samples.

```{r}

#| label: format-otu-table-long-taxa

species_reads_long_taxa <- eDNA_long_data_removefailed %>%
  select(ASV, valid_name, reads, fullID) %>%
  mutate(across(c(ASV, valid_name, fullID), as.factor)) %>%
  distinct()

species_reads_long_taxa_grouped <- species_reads_long_taxa %>% 
  group_by(valid_name, fullID) %>% 
  summarize(reads = sum(reads, na.rm = TRUE), .groups = "drop")

species_reads_wide_taxa <- species_reads_long_taxa_grouped %>% 
  pivot_wider(names_from = fullID, 
              values_from = reads, 
              values_fill = list(reads = 0)) %>% 
  as.data.frame()

#order names alphabetically
species_reads_wide_taxa<- species_reads_wide_taxa[order(species_reads_wide_taxa$valid_name),] #order rows alphabetically
species_reads_wide_taxa <- species_reads_wide_taxa[,order(colnames(species_reads_wide_taxa))] #order cols alphabetically

#update row names to ASVs
rownames(species_reads_wide_taxa) <- NULL
species_reads_wide_taxa <- species_reads_wide_taxa %>% column_to_rownames(var="valid_name")

dim(species_reads_wide_taxa)
```

Now let's create our `otu_table` elements - one at a ASV level (in case we need it later on) and one at a taxa-level (which we'll use for compiling the `phyloseq` object).

```{r}
#| label: create-otu-table

phylo_asv <- otu_table(species_reads_wide, taxa_are_rows=TRUE)
phylo_asv_taxa <- otu_table(species_reads_wide_taxa, taxa_are_rows=TRUE)
```

It's useful to do some checks befor we compile our taxa-level `phyloseq` object to make sure everything aligns correctly. This is why we've performed the alphabetical ordering in the above steps.

```{r}
#| label: check-phyloseq-taxa

# Check if colnames in asv object match meta rownames
list1<- colnames(phylo_asv_taxa)
list2 <- rownames(phylo_samples)
which(list1 != list2) #should be NULL

# Check if species names match in asv object and taxa object
list1<- rownames(phylo_asv_taxa)
list2 <- rownames(phylo_tax)
which(list1 != list2) #should be NULL
```

```{r}
#| label: compile-phyloseq

phylo_eDNA <- phyloseq(phylo_asv_taxa, phylo_tax, phylo_samples)

# Remove taxa wihout occurances from the data
phylo_eDNA <- subset_taxa(phylo_eDNA, taxa_sums(phylo_eDNA) > 0)
```

Let's take a look at what the `phyloseq` object looks like.

```{r}
#| label: look-phyloseq

head(sample_names(phylo_eDNA)) #check sample names
head(rank_names(phylo_eDNA)) #check taxonomic information
head(sample_variables(phylo_eDNA)) #check meta data 

phylo_eDNA
microbiome::summarize_phyloseq(phylo_eDNA) #full summary
```

## Decontamination

### Visualizing contaminants

Let's produce some figures to visualize potential contaminates present in our negative controls.

We need to subset the negative control samples first.

```{r}
#| label: subset-negatives

phylo_neg_controls = subset_samples(phylo_eDNA, sampleType == "field-control" | sampleType == "lab-negative-control") 

# Clean out taxa/ASV columns that are no longer present.
phylo_neg_controls <- prune_taxa(taxa_sums(phylo_neg_controls) > 0, phylo_neg_controls)
```

Let's check the distribution of taxa in the negative controls.

```{r}
#| label: abundance-negatives

plot(sort(taxa_sums(phylo_neg_controls), TRUE), type="h")
```

It might be useful to view contaminates by primer. Let's look at 18S first.

```{r}
#| label: 18S-negatives-abundance

phylo_neg_18S <- subset_samples(phylo_neg_controls, primer == "18S")

(contam_18S_plot<- plot_bar(phylo_neg_18S, "fullID", fill = "phylum") +
  facet_wrap(.~ primer)+
  theme(legend.position = 'none'))
```

And now for CO1.

```{r}
#| label: CO1-negatives-abundance

phylo_neg_CO1 <- subset_samples(phylo_neg_controls, primer == "CO1")

(contam_CO1_plot<- plot_bar(phylo_neg_CO1, "fullID", fill = "phylum") +
  facet_wrap(.~ primer) +
  theme(legend.position = 'none'))
```

We can also find taxa that are shared among a certain number of negative controls. As an example, we check which taxa are present in more than five samples.

```{r}
#| label: negatives-shared

controls_shared_taxa = filter_taxa(phylo_neg_controls, 
                                   function(x) sum(x >= 1) == (5), TRUE)
plot_bar(controls_shared_taxa, "fullID", fill = "valid_name")
```

We can also visualize our original PCR plates (if we have variables in our metadata which state the position of samples on the plates).

First, calculate overall read depth across samples and this to our sample data.

```{r}
#| label: calculate-sample-depths

#Calculate read depths
sample_depths <- microbiome::readcount(phylo_eDNA)
sample_depths_df <- as.data.frame(sample_depths)
sample_depths_df <- tibble::rownames_to_column(sample_depths_df, "fullID")
head(sample_depths_df)

# add depths to sample data
sample_data <- sample_data(phylo_eDNA)
sample_data[,"depth"] <- sample_depths
```

Let's look at CO1 first.

```{r}
#| label: PCR-plate-CO1

# Filter out NA values
sample_data_CO1 <- sample_data[!is.na(sample_data$plate_row_CO1), ]

# Reorder levels of plate_row_CO1
sample_data_CO1$plate_row_CO1 <- factor(sample_data_CO1$plate_row_CO1, 
                                        levels = rev(LETTERS[1:8]))

# Relabel PCR plates
sample_data_CO1$batch_PCR_CO1 <- recode(sample_data_CO1$batch_PCR_CO1,
                                    "1" = "SW-CO1",
                                    "2" = "SCH-CO1",
                                    "3" = "SW-18S",
                                    "4" = "SCH-18S",
                                    "5" = "NW-18S",
                                    "6" = "NE-18S",
                                    "7" = "CN-18S",
                                    "8" = "NW-CO1",
                                    "9" = "NE-CO1",
                                    "10" = "CN-CO1")

# Plot with facet_wrap and circles representing concentrations - CO1 first
pcr_plate_CO1 <- ggplot(sample_data_CO1, aes(x = plate_col_CO1, y = plate_row_CO1)) +
  geom_tile(aes(fill = sampleType), color = "white", alpha = 0.7) +
  geom_point(aes(size = depth), shape = 21, fill = "black", color = "black") + # Fill circles with black
  scale_fill_manual(values = c("field-control" = "brown", 
                               "lab-extraction-control" = "red",
                               "lab-negative-control" = "pink", 
                               "lab-positive-control" = "seagreen",
                               "sample" ="white", 
                               "plate-replicate" = "lightblue", 
                               "failed-repeat" = "white", 
                               "run-replicate" = "lightblue4")) + 
  scale_size_continuous(range = c(2, 10)) +  # Adjust circle sizes as needed
  labs(title = "Virtual PCR Plates",
       x = NULL,
       y = NULL,
       fill = "Control Type",
       size = "# of reads per sample") +
  theme_bw() +
  theme(legend.position = "right") + # Adjust legend position if needed
  facet_wrap(~ batch_PCR_CO1, ncol = 2, scales = "free") + # Facet by plate variable, 2 plates per row
  scale_x_continuous(breaks = 1:12)  # Discrete x-axis from 1 to 12

pcr_plate_CO1
```

And now for 18S.

```{r}
#| label: PCR-plate-18S

# Filter out NA values
sample_data_18S <- sample_data[!is.na(sample_data$plate_row_18S), ]

# Reorder levels of plate_row_18S
sample_data_18S$plate_row_18S <- factor(sample_data_18S$plate_row_18S, levels = rev(LETTERS[1:8]))

# Relabel PCR plates
sample_data_18S$batch_PCR_18S <- recode(sample_data_18S$batch_PCR_18S,
                                    "1" = "SW-CO1",
                                    "2" = "SCH-CO1",
                                    "3" = "SW-18S",
                                    "4" = "SCH-18S",
                                    "5" = "NW-18S",
                                    "6" = "NE-18S",
                                    "7" = "CN-18S",
                                    "8" = "NW-CO1",
                                    "9" = "NE-CO1",
                                    "10" = "CN-CO1")

# Plot with facet_wrap and circles representing concentrations - 18S first
pcr_plate_18S <- ggplot(sample_data_18S, aes(x = plate_col_18S, y = plate_row_18S)) +
  geom_tile(aes(fill = sampleType), color = "white", alpha = 0.7) +
  geom_point(aes(size = depth), shape = 21, fill = "black", color = "black") + # Fill circles with black
  scale_fill_manual(values = c("field-control" = "brown", 
                               "lab-extraction-control" = "red",
                               "lab-negative-control" = "pink", 
                               "lab-positive-control" = "seagreen",
                               "sample" ="white", 
                               "plate-replicate" = "lightblue", 
                               "failed-repeat" = "white", 
                               "run-replicate" = "lightblue4")) + 
  scale_size_continuous(range = c(2, 10)) +  # Adjust circle sizes as needed
  labs(title = "Virtual PCR Plates",
       x = NULL,
       y = NULL,
       fill = "Control Type",
       size = "# of reads per sample") +
  theme_bw() +
  theme(legend.position = "right") + # Adjust legend position if needed
  facet_wrap(~ batch_PCR_18S, ncol = 2, scales = "free") + # Facet by plate variable, 2 plates per row
  scale_x_continuous(breaks = 1:12)  # Discrete x-axis from 1 to 12

pcr_plate_18S
```

### Removing contaminants

We can see from our plots we have some contamination. We deal with this next.

There are multiple ways to deal with contamination. Here, we take the approach that if the abundance of a taxa in control sample is larger than abundance in main sample, it will be treated as contaminant and be removed.

We'll remove taxa in stages: from sites (using the field controls), extraction batch (using the extraction controls) then PCR batch (using the PCR controls). This prevents unnecessary blanket removal across the whole data set.

Let's set-up the site level decontamination function called `remove_field_control_taxa_abund` first.

```{r}

remove_field_control_taxa_abund <- function(phylo_object) {
  
  # Extract sample data
  sample_data <- sample_data(phylo_object)
  
  # Get unique localityIDs
  unique_localityIDs <- unique(sample_data$localityID[!is.na(sample_data$localityID)])
  
  # Check if unique_localityIDs is empty
  if (length(unique_localityIDs) == 0) {
    stop("No localityIDs found in sample data.")
  }
  
  # Initialize variable to count OTUs removed
  otus_removed <- data.frame(localityID = character(), OTU = character(), reads_removed = integer(), stringsAsFactors = FALSE)
  
  # Initialize filtered OTU table
  otu_table_filtered <- otu_table(phylo_object)
  
  # Check if the OTU table is empty
  if (dim(otu_table_filtered)[1] == 0 || dim(otu_table_filtered)[2] == 0) {
    stop("OTU abundance data is empty.")
  }
  
  print("Original dimensions of otu_table_filtered:")
  print(dim(otu_table_filtered))
  
  # Initialize list to store removed OTUs for each sample
  otus_removed_reads <- data.frame()
  otus_removed_reads_total <- data.frame()
  
  # Loop through each localityID
  for (locality in unique_localityIDs) {
    
    # Get field control sample fullIDs for the current locality
    field_control_samples <- unique(sample_data$fullID[sample_data$localityID == locality & grepl("-C_CO1$|-C_18S$", sample_data$fullID)])
    
    # If there are more than 2 field control samples for the current locality, print a message and skip
    if (length(field_control_samples) > 2) {
      cat("More than 1 field control samples found for site:", locality, "\n")
      next
    }
    
    # If there are no field control samples for the current locality, print a message and skip
    if (length(field_control_samples) < 2) {
      cat("No field control samples found for site:", locality, "\n")
      next
    }
    
    # Get indices of field control and samples
    control_indices <- which(sample_data$fullID %in% field_control_samples)
    sample_indices <- which(sample_data$localityID == locality & !sample_data$fullID %in% field_control_samples)
    
    # If no control indices found
    if (length(control_indices) == 0) {
      cat("No control indices found for site:", locality, "\n")
      next
    }
    
    # If no sample indices found
    if (length(sample_indices) == 0) {
      cat("No sample indices found for site:", locality, "\n")
      next
    }
    
    # Calculate mean abundance of each OTU in the control and samples
    control_abundance <- rowMeans(otu_table_filtered[, control_indices])
    sample_abundance <- rowMeans(otu_table_filtered[, sample_indices])
    
    # Identify potential contaminants based on higher abundance in control than samples
    contaminants <- rownames(otu_table_filtered)[control_abundance > sample_abundance]
    
    if (length(contaminants) > 0) {
      cat(length(contaminants), "contaminant taxa found for site:", locality, "\n")
    } else {
      cat("No contaminants found for site:", locality, "\n")
      next
    }
    
    #Get original number of reads in samples for taxa with above minReads reads in controls
    #bind the previous output
    if (length(otus_removed_reads) > 0) {
      otus_removed_reads_total <- rbind(otus_removed_reads, otus_removed_reads_total)
    }
    
    #get reads
    otus_removed_reads <- otu_table_filtered[contaminants,  sample_indices] %>% reshape2::melt()
    colnames(otus_removed_reads) <- c("taxa", "fullID", "reads_removed")
    
    # Set read count of OTUs in real samples to zero if reads in field control samples exceed 100
    otu_table_filtered[contaminants, sample_indices] <- 0
    
  }
  
  # Print OTUs removed
  cat("Reads removed:", sum(otus_removed_reads_total$reads_removed),"; Samples filtered:", length(unique(otus_removed_reads_total$fullID)), "\n")
  
  # Print new dimensions
  print("New dimensions of otu_table_filtered:")
  print(dim(otu_table_filtered))
  
  # Remove controls from the dataset
  control_samples<- sample_data$fullID[grepl("-C_CO1$|-C_18S$", sample_data$fullID)] # Build list of names of control samples
  otu_table_filtered <- otu_table_filtered[, -which(colnames(otu_table_filtered) %in% control_samples)] # Remove control samples from otu_table
  sample_data_filtered <- sample_data[!sample_data$fullID %in% control_samples, ] # Remove control samples from sample_data
  
  # Return the modified phyloseq object
  modified_phylo <- phyloseq(otu_table_filtered, tax_table(phylo_object), sample_data_filtered)
  
  # Remove taxa with zero reads removed in otus_removed_reads_total
  otus_removed_reads_total <- otus_removed_reads_total %>% subset(otus_removed_reads_total$reads_removed != 0)
  
  # Return the modified phyloseq object and otus_removed dataframe
  return(list(modified_phylo, otus_removed_reads_total))
}
```

Next, the function for extraction level decontamination called `remove_extract_control_taxa_abund`.

```{r}

remove_extract_control_taxa_abund <- function(phylo_object) {
  
  # Extract sample data
  sample_data <- sample_data(phylo_object)
  
  # Get unique batches
  unique_batches <- unique(sample_data$batch_extraction[!is.na(sample_data$batch_extraction)])
  
  # Check if unique_localityIDs is empty
  if (length(unique_batches) == 0) {
    stop("No DNA extraction batches found in sample data.")
  }
  
  # Initialize variable to count OTUs removed
  otus_removed <- data.frame(batch = character(), OTU = character(), reads_removed = integer(), stringsAsFactors = FALSE)
  
  # Initialize filtered OTU table
  otu_table_filtered <- otu_table(phylo_object)
  
  # Check if the OTU table is empty
  if (dim(otu_table_filtered)[1] == 0 || dim(otu_table_filtered)[2] == 0) {
    stop("OTU abundance data is empty.")
  }
  
  print("Original dimensions of otu_table_filtered:")
  print(dim(otu_table_filtered))
  
  # Initialize list to store removed OTUs for each sample
  otus_removed_reads <- data.frame()
  otus_removed_reads_total <- data.frame()
  
  # Loop through each localityID
  for (batch in unique_batches) {
    
    # Get extract control sample for the current batch
    dna_extract_control_samples <- unique(sub("_[^_]*$", "", sample_data$fullID[sample_data$sampleType == "lab-extraction-control" 
                                                                                & sample_data$batch_extraction == batch]))
    
    # If there are more than 2 field control samples for the current locality, print a message and skip
    if (length(dna_extract_control_samples) > 2) {
      cat("More than 1 DNA extract control sample found in batch:", batch, "\n")
      next
    }
    
    # If there are no field control samples for the current locality, print a message and skip
    if (length(dna_extract_control_samples) < 1) {
      cat("No DNA extract control samples found in batch:", batch, "\n")
      next
    }
    
    # Get indices of dna extract control and samples
    control_indices <- which(sample_data$fieldID %in% dna_extract_control_samples)
    sample_indices <- which(sample_data$batch_extraction == batch & !sample_data$fullID %in% dna_extract_control_samples)
    
    # If no control indices found
    if (length(control_indices) == 0) {
      cat("No control indices found for batch:", batch, "\n")
      next
    }
    
    # If no sample indices found
    if (length(sample_indices) == 0) {
      cat("No sample indices found for batch:", batch, "\n")
      next
    }
    
    # Calculate mean abundance of each OTU in the control and samples
    control_abundance <- rowMeans(otu_table_filtered[, control_indices])
    sample_abundance <- rowMeans(otu_table_filtered[, sample_indices])
    
    # Identify potential contaminants based on higher abundance in control than samples
    contaminants <- rownames(otu_table_filtered)[control_abundance > sample_abundance]
    
    if (length(contaminants) > 0) {
      cat(length(contaminants), "contaminant taxa found for site:", batch, "\n")
    } else {
      cat("No contaminants found for site:", batch, "\n")
      next
    }
    
    #Get original number of reads in samples for taxa with above minReads reads in controls
    #bind the previous output
    if (length(otus_removed_reads) > 0) {
      otus_removed_reads_total <- rbind(otus_removed_reads, otus_removed_reads_total)
    }
    
    #get reads
    otus_removed_reads <- otu_table_filtered[contaminants,  sample_indices] %>% reshape2::melt()
    colnames(otus_removed_reads) <- c("taxa", "fullID", "reads_removed")
    
    # Set read count of OTUs in real samples to zero if reads in field control samples exceed 100
    otu_table_filtered[contaminants, sample_indices] <- 0
    
    # Remove the control samples from the phylo object
    otu_table_filtered <- otu_table_filtered[-control_indices]
    
  }
  
  # Print OTUs removed
  cat("Reads removed:", sum(otus_removed_reads_total$reads_removed),"; Samples filtered:", length(unique(otus_removed_reads_total$fullID)), "\n")
  
  # Remove controls from the dataset
  control_samples <- unique(sub("_[^_]*$", "", sample_data$fullID[sample_data$sampleType == "lab-extraction-control"])) # Build list of names of control samples
  control_samples <- c(paste0(control_samples, "_CO1"), paste0(control_samples, "_18S"))  # Create a list of control samples by appending "_CO1" and "_18S" to each ID
  otu_table_filtered <- otu_table_filtered[, -which(colnames(otu_table_filtered) %in% control_samples)] # Remove control samples from otu_table
  sample_data_filtered <- sample_data[!sample_data$fullID %in% control_samples, ] # Remove control samples from sample_data
  
  # Print new dimensions
  print("New dimensions of otu_table_filtered:")
  print(dim(otu_table_filtered))
  
  # Return the modified phyloseq object
  modified_phylo <- phyloseq(otu_table_filtered, tax_table(phylo_object), sample_data_filtered)
  
  # Remove taxa with zero reads removed in otus_removed_reads_total
  otus_removed_reads_total <- otus_removed_reads_total %>% subset(otus_removed_reads_total$reads_removed != 0)
  
  # Return the modified phyloseq object and otus_removed data frame
  return(list(modified_phylo, otus_removed_reads_total))
}
```

And finally, the function for PCR level decontamination. We have to do this by marker, considering the PCRs are different for each.

The 18S function is called `remove_PCR_control_taxa_18S_abund`.

```{r}

remove_PCR_control_taxa_18S_abund <- function(phylo_object) {
  
  # Extract sample data
  sample_data <- sample_data(phylo_object)
  
  # Get unique batches
  unique_batches <- unique(sample_data$batch_PCR_18S[!is.na(sample_data$batch_PCR_18S)])
  
  # Check if unique_localityIDs is empty
  if (length(unique_batches) == 0) {
    stop("No PCR (18S) batches found in sample data.")
  }
  
  # Initialize variable to count OTUs removed
  otus_removed <- data.frame(batch = character(), OTU = character(), reads_removed = integer(), stringsAsFactors = FALSE)
  
  # Initialize filtered OTU table
  otu_table_filtered <- otu_table(phylo_object)
  
  # Check if the OTU table is empty
  if (dim(otu_table_filtered)[1] == 0 || dim(otu_table_filtered)[2] == 0) {
    stop("OTU abundance data is empty.")
  }
  
  print("Original dimensions of otu_table_filtered:")
  print(dim(otu_table_filtered))
  
  # Initialize list to store removed OTUs for each sample
  otus_removed_reads <- data.frame()
  otus_removed_reads_total <- data.frame()
  
  # Loop through each batch_PCR_18S
  for (batch in unique_batches) {
    
    # Get extract control samples for the current batch
    pcr_18S_control_samples <- unique(sub("_[^_]*$", "", sample_data$fullID[sample_data$sampleType == "lab-negative-control" 
                                                                            & sample_data$batch_PCR_18S == batch]))
    pcr_18S_control_samples <- na.omit(pcr_18S_control_samples)
    
    # If there are no PCR 18S control samples for the current batch, print a message and skip
    if (length(pcr_18S_control_samples) < 1) {
      cat("No PCR (18S) control samples found in batch:", batch, "\n")
      next
    }
    
    # Get indices of PCR (18S) control and samples
    control_indices <- which(sample_data$fieldID %in% pcr_18S_control_samples)
    sample_indices <- which(sample_data$batch_PCR_18S == batch & !sample_data$fullID %in% pcr_18S_control_samples)
    
    # If no control indices found
    if (length(control_indices) == 0) {
      cat("No control indices found for batch:", batch, "\n")
      next
    }
    
    # If no sample indices found
    if (length(sample_indices) == 0) {
      cat("No sample indices found for batch:", batch, "\n")
      next
    }
    
    # If there are more than PCR 18S control samples for the current locality, loop through the controls
    if (length(pcr_18S_control_samples) > 2) {
      cat("More than 1 PCR (18S) control sample found in batch", batch, ". Now printing individual controls:", "\n")
      
      for (control in control_indices) {
        
        # Calculate mean abundance of each OTU in the control and samples
        control_abundance <- rowMeans(otu_table_filtered[, control])
        sample_abundance <- rowMeans(otu_table_filtered[, sample_indices])
        
        #check there are OTUs with reads above minReads in the field control samples
        control_sum_above_minReads <- rowSums(otu_table_filtered[, control]) > 100
        
        if (any(control_sum_above_minReads)) {
          contaminants <- rownames(otu_table_filtered[control_abundance > sample_abundance])
          cat("Batch", batch, ":", sum(control_sum_above_minReads == T), "contaminant taxa found for control indices", control, "\n")
        } else {
          cat("Batch", batch, ": No contaminants found for control indices", control, "\n")
          next
        }
        
        #Get original number of reads in samples for taxa with above minReads reads in controls
        #bind the previous output
        if (length(otus_removed_reads) > 0) {
          otus_removed_reads_total <- rbind(otus_removed_reads, otus_removed_reads_total)
        }
        
        #get reads
        otus_removed_reads <- otu_table_filtered[contaminants,  sample_indices] %>% reshape2::melt()
        colnames(otus_removed_reads) <- c("taxa", "fullID", "reads_removed")
        
        # Set read count of OTUs in real samples to zero if reads in field control samples exceed 100
        otu_table_filtered[contaminants, sample_indices] <- 0
        
        # Remove the control samples from the phylo object
        otu_table_filtered <- otu_table_filtered[-control]
      }
    } else {
      
      #check there are OTUs with reads above minReads in the field control samples
      control_sum_above_minReads <- rowSums(otu_table_filtered[, control_indices]) > minReads
      
      if (any(control_sum_above_minReads)) {
        contaminants <- rownames(otu_table_filtered[rowSums(otu_table_filtered[, control_indices]) > minReads])
        cat(sum(control_sum_above_minReads == T), "contaminant taxa found for batch", batch, "\n")
      } else {
        cat("No contaminants found for batch", batch, "\n")
        next
      }
      
      #Get original number of reads in samples for taxa with above minReads reads in controls
      #bind the previous output
      if (length(otus_removed_reads) > 0) {
        otus_removed_reads_total <- rbind(otus_removed_reads, otus_removed_reads_total)
      }
      
      #get reads
      otus_removed_reads <- otu_table_filtered[contaminants,  sample_indices] %>% reshape2::melt()
      colnames(otus_removed_reads) <- c("taxa", "fullID", "reads_removed")
      
      # Set read count of OTUs in real samples to zero if reads in field control samples exceed 100
      otu_table_filtered[contaminants, sample_indices] <- 0
      
      # Remove the control samples from the phylo object
      otu_table_filtered <- otu_table_filtered[-control_indices]
    }
  }
  
  # Print OTUs removed
  cat("Reads removed:", sum(otus_removed_reads_total$reads_removed),"; Samples filtered:", length(unique(otus_removed_reads_total$fullID)), "\n")
  
  # Get list of control samples
  control_samples <- unique(sub("_[^_]*$", "", sample_data$fullID[sample_data$sampleType == "lab-negative-control"])) # Build list of names of control samples
  control_samples <- c(paste0(control_samples, "_18S")) # Create a list of control samples by appending "_CO1" to each ID
  
  # Check control samples to filtered otu table (will be more in the list than reality because some aren't _CO1)
  print(control_samples)
  print(which(control_samples %in% colnames(otu_table_filtered)))
  
  # Remove control samples from otu and sample table
  otu_table_filtered <- otu_table_filtered[, -which(colnames(otu_table_filtered) %in% control_samples)] # Remove control samples from otu_table
  sample_data_filtered <- sample_data[!sample_data$fullID %in% control_samples, ] # Remove control samples from sample_data
  
  # Print new dimensions
  print("New dimensions of otu_table_filtered:")
  print(dim(otu_table_filtered))
  
  # Return the modified phyloseq object
  modified_phylo <- phyloseq(otu_table_filtered, tax_table(phylo_object), sample_data_filtered)
  
  # Remove taxa with zero reads removed in otus_removed_reads_total
  otus_removed_reads_total <- otus_removed_reads_total %>% subset(otus_removed_reads_total$reads_removed != 0)
  
  # Return the modified phyloseq object and otus_removed dataframe
  return(list(modified_phylo, otus_removed_reads_total))
}

```

And the CO1 function is called `remove_PCR_control_taxa_CO1_abund`.

```{r}
### PCR negative decontamination (CO1)
remove_PCR_control_taxa_CO1_abund <- function(phylo_object) {
  
  # Extract sample data
  sample_data <- sample_data(phylo_object)
  
  # Get unique batches
  unique_batches <- unique(sample_data$batch_PCR_CO1[!is.na(sample_data$batch_PCR_CO1)])
  
  # Check if unique_localityIDs is empty
  if (length(unique_batches) == 0) {
    stop("No PCR (CO1) batches found in sample data.")
  }
  
  # Initialize variable to count OTUs removed
  otus_removed <- data.frame(batch = character(), OTU = character(), reads_removed = integer(), stringsAsFactors = FALSE)
  
  # Initialize filtered OTU table
  otu_table_filtered <- otu_table(phylo_object)
  
  # Check if the OTU table is empty
  if (dim(otu_table_filtered)[1] == 0 || dim(otu_table_filtered)[2] == 0) {
    stop("OTU abundance data is empty.")
  }
  
  print("Original dimensions of otu_table_filtered:")
  print(dim(otu_table_filtered))
  
  # Initialize list to store removed OTUs for each sample
  otus_removed_reads <- data.frame()
  otus_removed_reads_total <- data.frame()
  
  # Loop through each batch_PCR_CO1
  for (batch in unique_batches) {
    
    # Get extract control samples for the current batch
    pcr_CO1_control_samples <- unique(sub("_[^_]*$", "", sample_data$fullID[sample_data$sampleType == "lab-negative-control" 
                                                                            & sample_data$batch_PCR_CO1 == batch]))
    pcr_CO1_control_samples <- na.omit(pcr_CO1_control_samples)
    
    # If there are no PCR CO1 control samples for the current batch, print a message and skip
    if (length(pcr_CO1_control_samples) < 1) {
      cat("No PCR (CO1) control samples found in batch:", batch, "\n")
      next
    }
    
    # Get indices of PCR (CO1) control and samples
    control_indices <- which(sample_data$fieldID %in% pcr_CO1_control_samples)
    sample_indices <- which(sample_data$batch_PCR_CO1 == batch & !sample_data$fullID %in% pcr_CO1_control_samples)
    
    # If no control indices found
    if (length(control_indices) == 0) {
      cat("No control indices found for batch:", batch, "\n")
      next
    }
    
    # If no sample indices found
    if (length(sample_indices) == 0) {
      cat("No sample indices found for batch:", batch, "\n")
      next
    }
    
    # If there are more than PCR CO1 control samples for the current locality, loop through the controls
    if (length(pcr_CO1_control_samples) > 2) {
      cat("More than 1 PCR (CO1) control sample found in batch", batch, ". Now printing individual controls:", "\n")
      
      for (control in control_indices) {
        
        # Calculate mean abundance of each OTU in the control and samples
        control_abundance <- rowMeans(otu_table_filtered[, control])
        sample_abundance <- rowMeans(otu_table_filtered[, sample_indices])
        
        #check there are OTUs with reads above minReads in the field control samples
        control_sum_above_minReads <- rowSums(otu_table_filtered[, control]) > 100
        
        if (any(control_sum_above_minReads)) {
          if (any(control_abundance > sample_abundance)) {
            contaminants <- rownames(otu_table_filtered[control_abundance > sample_abundance])
            cat("Batch", batch, ":", sum(control_sum_above_minReads == TRUE), "contaminant taxa found for control indices", control, "\n")
          } else {
            cat("Batch", batch, ": Control abundance is not greater than sample abundance. OTU table would be empty.\n")
            next
          }
        } else {
          cat("Batch", batch, ": No contaminants found for control indices", control, "\n")
          next
        }
        
        #Get original number of reads in samples for taxa with above minReads reads in controls
        #bind the previous output
        if (length(otus_removed_reads) > 0) {
          otus_removed_reads_total <- rbind(otus_removed_reads, otus_removed_reads_total)
        }
        
        #get reads
        otus_removed_reads <- otu_table_filtered[contaminants,  sample_indices] %>% reshape2::melt()
        colnames(otus_removed_reads) <- c("taxa", "fullID", "reads_removed")
        
        # Set read count of OTUs in real samples to zero if reads in field control samples exceed 100
        otu_table_filtered[contaminants, sample_indices] <- 0
        
        # Remove the control samples from the phylo object
        otu_table_filtered <- otu_table_filtered[-control]
      }
    } else {
      
      #check there are OTUs with reads above minReads in the field control samples
      control_sum_above_minReads <- rowSums(otu_table_filtered[, control_indices]) > minReads
      
      if (any(control_sum_above_minReads)) {
        contaminants <- rownames(otu_table_filtered[rowSums(otu_table_filtered[, control_indices]) > minReads])
        cat(sum(control_sum_above_minReads == T), "contaminant taxa found for batch", batch, "\n")
      } else {
        cat("No contaminants found for batch", batch, "\n")
        next
      }
      
      #Get original number of reads in samples for taxa with above minReads reads in controls
      #bind the previous output
      if (length(otus_removed_reads) > 0) {
        otus_removed_reads_total <- rbind(otus_removed_reads, otus_removed_reads_total)
      }
      
      #get reads
      otus_removed_reads <- otu_table_filtered[contaminants,  sample_indices] %>% reshape2::melt()
      colnames(otus_removed_reads) <- c("taxa", "fullID", "reads_removed")
      
      # Set read count of OTUs in real samples to zero if reads in field control samples exceed 100
      otu_table_filtered[contaminants, sample_indices] <- 0
      
      # Remove the control samples from the phylo object
      otu_table_filtered <- otu_table_filtered[-control_indices]
    }
  }
  
  # Print OTUs removed
  cat("Reads removed:", sum(otus_removed_reads_total$reads_removed),"; Samples filtered:", length(unique(otus_removed_reads_total$fullID)), "\n")
  
  # Get list of control samples
  control_samples <- unique(sub("_[^_]*$", "", sample_data$fullID[sample_data$sampleType == "lab-negative-control"])) # Build list of names of control samples
  control_samples <- c(paste0(control_samples, "_CO1")) # Create a list of control samples by appending "_CO1" to each ID
  
  # Check control samples to filtered otu table (will be more in the list than reality because some aren't _CO1)
  print(control_samples)
  print(which(control_samples %in% colnames(otu_table_filtered)))
  
  # Remove control samples from otu and sample table
  otu_table_filtered <- otu_table_filtered[, -which(colnames(otu_table_filtered) %in% control_samples)] # Remove control samples from otu_table
  sample_data_filtered <- sample_data[!sample_data$fullID %in% control_samples, ] # Remove control samples from sample_data
  
  # Print new dimensions
  print("New dimensions of otu_table_filtered:")
  print(dim(otu_table_filtered))
  
  # Return the modified phyloseq object
  modified_phylo <- phyloseq(otu_table_filtered, tax_table(phylo_object), sample_data_filtered)
  
  # Remove taxa with zero reads removed in otus_removed_reads_total
  otus_removed_reads_total <- otus_removed_reads_total %>% subset(otus_removed_reads_total$reads_removed != 0)
  
  # Return the modified phyloseq object and otus_removed data frame
  return(list(modified_phylo, otus_removed_reads_total))
}
```

Time to run the decontamination functions one after the other, using the output of the first as the input for the last.

```{r}

minReads = 100 #to identify a contaminant

#Call remove_field_control_taxa_abund() function (site)
result_field_abun <- remove_field_control_taxa_abund(phylo_eDNA)
phylo_eDNA_filtered_field_abun <- result_field_abun[[1]]
otus_removed_field_abun <- result_field_abun[[2]]

#Call remove_extract_control_taxa_abund() function (extracts)
result_extract_abun <- remove_extract_control_taxa_abund(phylo_eDNA_filtered_field_abun)
phylo_eDNA_filtered_extract_abun <- result_extract_abun[[1]]
otus_removed_extract_abun <- result_extract_abun[[2]]

#Call remove_PCR_CO1_control_taxa_abund() function (PCR)
result_PCR_CO1_abun <- remove_PCR_control_taxa_CO1_abund(phylo_eDNA_filtered_extract_abun)
phylo_eDNA_filtered_PCR_CO1_abun <- result_PCR_CO1_abun[[1]]
otus_removed_PCR_CO1_abun <- result_PCR_CO1_abun[[2]]

#all remove_PCR_18S_control_taxa_abund() function (PCR)
result_PCR_18S_abun <- remove_PCR_control_taxa_18S_abund(phylo_eDNA_filtered_PCR_CO1_abun)
phylo_eDNA_filtered_PCR_18S_abun <- result_PCR_18S_abun[[1]]
otus_removed_PCR_18S_abun <- result_PCR_18S_abun[[2]]

# rename file data
phylo_eDNA_decontam <- phylo_eDNA_filtered_PCR_18S_abun

# remove taxa with no reads across all samples
phylo_eDNA_decontam <- subset_taxa(phylo_eDNA_decontam, taxa_sums(phylo_eDNA_decontam) > 0)
```

Let's compared our decontaminated data with the original.

We can see that control samples have now been removed from the data and 51 taxa have also been removed.

```{r}

phylo_eDNA
phylo_eDNA_decontam
```

We can also check to see how many samples are categorized under different sample types.

```{r}

table(meta(phylo_eDNA_decontam)$sampleType, useNA = "always") #sample type
table(meta(phylo_eDNA_decontam)$controlCheck, useNA = "always") #control type
table(meta(phylo_eDNA_decontam)$shorePosition, useNA = "always") #shore height
```

We need to tidy up the shore positions to high, low and open water only.

```{r}

sample_data(phylo_eDNA_decontam)$shorePosition[sample_data(phylo_eDNA_decontam)$shorePosition == "Mid "] <- "High"
sample_data(phylo_eDNA_decontam)$shorePosition[sample_data(phylo_eDNA_decontam)$shorePosition == "Tidal"] <- "Open Water"
sample_data(phylo_eDNA_decontam)$shorePosition[sample_data(phylo_eDNA_decontam)$shorePosition == "Subtidal"] <- "Open Water"
sample_data(phylo_eDNA_decontam)$shorePosition[sample_data(phylo_eDNA_decontam)$shorePosition == "Mid"] <- "High"

table(meta(phylo_eDNA_decontam)$shorePosition, useNA = "always") #shore height
```

We also don't require our positive controls now.

```{r}

phylo_eDNA_decontam <- subset_samples(phylo_eDNA_decontam, !(controlCheck == "positive")) 

table(meta(phylo_eDNA_decontam)$sampleType, useNA = "always") #sample type
```

## Filtering steps

### Removal of low read samples

It's good practice to remove any samples with the low read depth to prevent sampling bias driving ecological patterns. We are going to explore the distribution of read depths across samples to make a descision on a minimum depth threshold.

We first need to calculate read depth per sample.

```{r}

# Number of reads per sample (before filtering)
sample_depths_decontam <- microbiome::readcount(phylo_eDNA_decontam) %>%
  as.data.frame() %>%
  tibble::rownames_to_column(var = "fullID")

colnames(sample_depths_decontam) <- c("fullID", "reads_prefilter")

# Calculate the total ASVs by counting the number of rows 
num_asvs_vec <- c(nrow(phyloseq::otu_table(phylo_eDNA_decontam)))  
names(num_asvs_vec)[1] <- "no.taxa" 
num_asvs_vec

# Total number asvs per sample 
asv_sum <- as.data.frame(colSums(phyloseq::otu_table(phylo_eDNA_decontam)!=0))  

#Extract sample meta data as a separate R object 
abundance_metadf <- phyloseq::sample_data(phylo_eDNA_decontam) 

#Check if vector of sample_depths & asv_sum has the same order as our metadata rows
identical(sample_depths_decontam$fullID,row.names(abundance_metadf)) 
identical(row.names(asv_sum),row.names(abundance_metadf))  

#Add sample depths and number of ASVs to metadata data frame 
abundance_metadf[,"depth"] <- sample_depths_decontam$reads_prefilter 
abundance_metadf[,"ASVs"] <- asv_sum
```

Check the distribution of sample reads.

```{r}

pcr_seq_depth_overall <- ggplot(abundance_metadf, aes(depth)) +
  geom_histogram(bins = 60, color = "grey36", fill = "white") +   
  geom_vline(xintercept = 3000, lty = 2,color = "orange") + # threshold
  scale_x_log10() +   
  labs(x = "# Reads", y = "# Samples") +
  theme_bw() +
  theme(panel.grid = element_blank())

pcr_seq_depth_overall 
```

Now let's flag samples with an acceptable (TRUE) or inacceptable (FALSE) sequencing depth. We should aim to keep at least 90% of samples.

```{r}

abundance_metadf$depth_ok <- ifelse(abundance_metadf$depth < 3000, F, T)

table(abundance_metadf$depth_ok[abundance_metadf$negCheckLogical=="FALSE"]) /
  nrow(abundance_metadf[abundance_metadf$negCheckLogical=="FALSE",])
```

A minimum threshold of 3000 reads looks like it could work well. Let's now filter our data set to remove any samples below 3000.

```{r}

sample_depths_object <- sample_sums(phylo_eDNA_decontam) # Calculate sample sequencing depths
min_reads <- 3000 # Define the minimum read threshold

# Prune samples with fewer than 3000 reads
phylo_eDNA_decontam_filtered <- prune_samples(sample_depths_object >= min_reads, phylo_eDNA_decontam)

#Check the number of samples before and after filtering
cat("Number of samples before filtering:", nsamples(phylo_eDNA_decontam), "\n")
cat("Number of samples after filtering:", nsamples(phylo_eDNA_decontam_filtered), "\n") #removed 19 samples
```

### Filtering for target functional groups (broad)

For this project, we were only interested in marine invertebrates and algae. So, we need to remove any other type of taxa. We'll start with a broad removal, then focus on species phyla.

Let's first examine what groups we have.

```{r}

unique(tax_table(phylo_eDNA_decontam_filtered)[, "adult"])
```

Define which groups not of interest.

```{r}

fish_and_small_stuff <- c("fish", 
                         "microplankton", 
                         "phytoplankton", 
                         "nanoplankton", 
                         "mixoplankton", 
                         "microbenthos")

other_unwanted_class <- c("Aves", 
                          "Mammalia", 
                          "Amphibia")

other_unwanted_order <- c("Squamata")
```

Now let's remove them from the relevant groups.

```{r}

phylo_eDNA_decontam_filtered <- subset_taxa(
  phylo_eDNA_decontam_filtered,
  !(adult %in% fish_and_small_stuff) & 
  !(class %in% other_unwanted_class) & 
  !(order %in% other_unwanted_order) 
)

# Rmove any taxa without occurances
phylo_eDNA_decontam_filtered <- subset_taxa(phylo_eDNA_decontam_filtered, taxa_sums(phylo_eDNA_decontam_filtered) > 0)


phylo_eDNA_decontam_filtered
```

### Filtering for target taxa (more specific)

Let's define which phyla we are interested in.

```{r}

target_phyla <-   c(
  "Acoela",
  "Amphioxus",
  "Annelida",
  "Arthropoda",
  "Brachiopoda",
  "Bryozoa",
  "Cephalochordata",
  "Chaetognatha",
  "Chlorophyta",
  "Chordata",
  "Cnidaria",
  "Crustacea",
  "Ctenophora",
  "Dinoflagellata",
  "Echiura",
  "Echinoderm",
  "Echinodermata",
  "Euglenophyta",
  "Gastrotricha",
  "Gnathostomulids",
  "Gyrista",
  "Hemichordata",
  "Kamptozoa",
  "Kinorhyncha",
  "Loricifera",
  "Mollusca",
  "Monogenea",
  "Myzostomida",
  "Nematoda",
  "Nemertinea",
  "Ochrophyta",
  "Orthonectida",
  "Phaeophyta",
  "Phoronida",
  "Placozoa",
  "Platyhelminthes",
  "Porifera",
  "Priapulida",
  "Pycnogonida",
  "Rhodophyta",
  "Sipunculida",
  "Thallophyta",
  "Tunicata",
  "Turbellaria",
  "Xenoturbella",
  "Xiphosura"
)
```

Now let's remove these from the data.

```{r}

phylo_eDNA_decontam_filtered <- subset_taxa(phylo_eDNA_decontam_filtered,                                              phylum %in% target_phyla)

# Rmove any taxa without occurances
phylo_eDNA_decontam_filtered <- subset_taxa(phylo_eDNA_decontam_filtered, taxa_sums(phylo_eDNA_decontam_filtered) > 0)
                                            
phylo_eDNA_decontam_filtered
```

Let's check to see how many taxa we have for invertebrates and macrophytes.

First, let's define our groups.

```{r}

invert_phyla <- c(
  "Acoela",
  "Amphioxus",
  "Annelida",
  "Arthropoda",
  "Brachiopoda",
  "Bryozoa",
  "Cephalochordata",
  "Chaetognatha",
  "Chordata",
  "Cnidaria",
  "Crustacea",
  "Echiura",
  "Echinoderm",
  "Echinodermata",
  "Gastrotricha",
  "Gnathostomulids",
  "Hemichordata",
  "Kamptozoa",
  "Kinorhyncha",
  "Loricifera",
  "Mollusca",
  "Monogenea",
  "Myzostomida",
  "Nematoda",
  "Nemertinea",
  "Orthonectida",
  "Phaeophyta",
  "Phoronida",
  "Placozoa",
  "Platyhelminthes",
  "Porifera",
  "Priapulida",
  "Pycnogonida",
  "Sipunculida",
  "Thallophyta",
  "Tunicata",
  "Xenoturbella",
  "Xiphosura"
)                    

algae_phyla <- c(
  "Chlorophyta",
  "Ctenophora",
  "Dinoflagellata",
  "Euglenophyta",
  "Gyrista",
  "Ochrophyta",
  "Phaeophyta",
  "Rhodophyta",
  "Thallophyta"
)
```

Let's look at invertebrates first.

```{r}

phylo_eDNA_decontam_filtered_invert <- subset_taxa(phylo_eDNA_decontam_filtered, phylum %in% invert_phyla)  

phylo_eDNA_decontam_filtered_invert   

print(paste("Invertebrate species account for", (737 / 1026) * 100, "% of detections."))
```

Now let's look at algae.

```{r}

phylo_eDNA_decontam_filtered_algae <- subset_taxa(phylo_eDNA_decontam_filtered, phylum %in% algae_phyla)  

phylo_eDNA_decontam_filtered_algae  

print(paste("Algae species account for", (289 / 1026) * 100, "% of detections."))
```

## Further quality control

Now we've got our decontaminated data, we can do a variety of quality control checks to make sure everything looks like it should.

### Set-up (functions and variables)

Let's set-up some functions we'll use later on to extract random pieces of information from our data.

```{r}

# Extract fieldID when adding TRUE or FALSE for replicate status
extract_middle <- function(x) {
  middle <- gsub("^[^-]+-([A-Za-z]{2}\\d{2}-\\d{2}).*_(18S|CO1)$", "\\1", x)
  return(middle)
}

# Extract site when adding TRUE or FALSE for replicate status
extract_site <- function(x) {
  # Extract the part after the first '-' and before the second '-' or '_' and remove anything after
  middle <- gsub("^[^-]+-([^-]+)-.*_(18S|CO1)$", "\\1", x)
  return(middle)
}


# Extract the middle part (including fieldID and primer)
extract_middle_all <- function(x) {
  # Extract the middle part of the string
  middle <- gsub("^[^-]+-(.+)$", "\\1", x)
  
  # Remove the term 'rep' and any following number
  middle <- gsub("rep\\d*", "", middle, ignore.case = TRUE)
  
  # Return the modified middle part
  return(middle)
}

# Modify the extract_middle function to extract the fieldID part for CO1 only groups (run 3)
extract_middle_CO1 <- function(x) {
  middle <- gsub("^[^-]+-([A-Za-z]{2}\\d{2}-\\d{2}).*_CO1$", "\\1", x)
  return(middle)
}
```

We need to set some variables for plots later.

```{r}

site_order <- c("Scourie", 
                "Rispond", 
                "Skerray", 
                "Murkle Bay", 
                "Portskerra", 
                "Borwick, Yesnaby", 
                "Sannick", 
                "Wick", #Scotland
                "Aberystwyth", 
                "Neyland", 
                "Broad Haven", 
                "Skomer Island", 
                "West Angle", 
                "Monkstone Point", 
                "Dale Jetty", 
                "Martin's Haven", #South Wales
                "Castlehead Rocks", 
                "Filey Brigg", 
                "Newton Point", 
                "Rumbling Kern", 
                "Scalby Mills", #Northumbria
                "Great Orme East", 
                "Little Orme", 
                "Menai Bridge", 
                "Porth Oer", 
                "Porth Swtan", 
                "Rhosneigr", #North Wales
                "Lizard Point", 
                "Looe", 
                "Sennen Cove", 
                "St Ives", 
                "Trevone") #Cornwall

control_order <- c("field-control", 
                   "lab-extraction-control", 
                   "lab-negative-control", 
                   "lab-positive-control", 
                   "sample", 
                   "run-replicate", 
                   "failed-repeat", 
                   "plate-replicate")

control_order_tidy <- c("Field control", 
                        "Lab extraction control", 
                        "Lab negative control", 
                        "Lab positive control", 
                        "Sample", 
                        "Run replicate", 
                        "Failed repeat", 
                        "Plate replicate")
```

### Explore read depths

Let's summarize the number of reads before and after decontamination and filtering. We calculated the pre-filtered data above, so now let's calculate the post filtered data and compare them.

```{r}

# Number of reads per sample (after decontamination and filtering)
sample_depths_decontam_filter <- microbiome::readcount(phylo_eDNA_decontam_filtered) %>% 
  as.data.frame() %>%
  tibble::rownames_to_column(var = "fullID")

colnames(sample_depths_decontam_filter) <- c("fullID", "reads_postfilter")

# Reads before and after decontamination together
all_depths <- left_join(sample_depths_decontam, 
                        sample_depths_decontam_filter, 
                        by = "fullID")

#add difference between start and end
all_depths$reads_removed <- all_depths$reads_prefilter - all_depths$reads_postfilter 

tail(all_depths)
```

Let's create some figures to visualize read depths before and after filtering. We are going to create some new data set for plotting.

We did the pre-filtered data above. Let's do the filtered data now.
